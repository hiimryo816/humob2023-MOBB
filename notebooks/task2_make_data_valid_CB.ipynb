{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2 \n",
    "- train #0-19999\n",
    "- valid #20000-22499\n",
    "- test #22500-24999\n",
    "\n",
    "uid 0~19999のデータフレームを作成してparquetで保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "# import datetime\n",
    "import os\n",
    "import gc\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import geobleu\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "#　擬似スコア\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import importlib\n",
    "from util import *\n",
    "import util\n",
    "\n",
    "from calc_metrices import *\n",
    "import calc_metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "  exp = 'ens104'\n",
    "  task = 'task2'\n",
    "  data = 'CB'\n",
    "  stage = \"valid\"\n",
    "  # candidate hypara. group window_size\n",
    "  window_size = 3\n",
    "  co_start_day = 0\n",
    "  co_end_day = 59\n",
    "\n",
    "  # co-visitation matrix\n",
    "  n_ago = 3\n",
    "  \n",
    "  # train test split. taskごとに要変更\n",
    "  start_uid = 20000\n",
    "  end_uid = 22499\n",
    "  train_start_day = 0\n",
    "  train_end_day = 59\n",
    "  test_start_day = 60\n",
    "  test_end_day = 74\n",
    "\n",
    "  # train, valid split. taskごとに要変更\n",
    "  model_train_start_day = 0\n",
    "  model_train_end_day = 40\n",
    "  model_valid_start_day = 41\n",
    "  model_valid_end_day = 59\n",
    "\n",
    "  seed = 123\n",
    "  debug = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = f'/root/humob/input/pkl/' #各自データがあるパスに変更の必要あり\n",
    "OUTPUT_DIR = f'/root/humob/pub/data/{Config.task}/{Config.data}/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "  os.makedirs(OUTPUT_DIR)\n",
    "if not os.path.exists(OUTPUT_DIR+Config.stage):\n",
    "  os.makedirs(OUTPUT_DIR+Config.stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : Dataset_load...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Processing : Dataset_load...')\n",
    "print('')\n",
    "\n",
    "# Load dataset\n",
    "df = dataset_load(Config.task, Config.debug, INPUT_DIR)\n",
    "\n",
    "train_weekend_list = [0, 1, 6, 7, 8, 13, 14, 20, 21, 27, 28, 29, 34, 35, 37, 41, 42, 48, 49, 50, 55, 56,]\n",
    "test_not_normal_day = [65, 66, 67, 72, 73]\n",
    "\n",
    "all_days = np.arange(df['d'].nunique())\n",
    "weekend_list = train_weekend_list + test_not_normal_day\n",
    "weekday_list = [day for day in all_days if day not in weekend_list]\n",
    "\n",
    "# preprocess\n",
    "df = extract_uid(df, Config.start_uid, Config.end_uid)\n",
    "df[\"wd\"] = df[\"d\"] % 7\n",
    "df['xy'] = df['x'].astype(str).str.zfill(3) + df['y'].astype(str).str.zfill(3)\n",
    "df['xy'] = df['xy'].astype(int)\n",
    "\n",
    "df = add_cumcount(df)\n",
    "df = add_group_xx(df, window_size=Config.window_size)\n",
    "df, drop_cols = add_lag_mesh(df, Config.n_ago)\n",
    "\n",
    "train, test = train_test_split_func(df, Config.train_start_day, Config.train_end_day, Config.test_start_day, Config.test_end_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = test.groupby([\"uid\", \"d\"]).head(1)[[\"uid\", \"d\", \"wd\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(train, test, train_df):\n",
    "    ### train期間の特徴量\n",
    "    # uid*dごとのログの長さ中央値\n",
    "    uid2log_len_median = train.groupby([\"uid\", \"d\"]).size().groupby(\"uid\").median().to_dict()\n",
    "\n",
    "    # uid*dごとのログの長さ平均値\n",
    "    uid2log_len_mean = train.groupby([\"uid\", \"d\"]).size().groupby(\"uid\").mean().to_dict()\n",
    "\n",
    "    # uid*dごとのログの長さ分散\n",
    "    uid2log_len_std = train.groupby([\"uid\", \"d\"]).size().groupby(\"uid\").std().to_dict()\n",
    "\n",
    "    # uid*dごとのログの長さ歪度\n",
    "    uid2log_len_skew = train.groupby([\"uid\", \"d\"]).size().groupby(\"uid\").skew().to_dict()\n",
    "\n",
    "    # uid*wdごとのログの長さ中央値\n",
    "    uid_wd2wd_log_len_median = train.groupby([\"uid\", \"wd\", \"d\"]).size().groupby([\"uid\",\"wd\"]).median().to_dict()\n",
    "\n",
    "    # uid*wdごとのログの長さ平均値\n",
    "    uid_wd2wd_log_len_mean = train.groupby([\"uid\", \"wd\", \"d\"]).size().groupby([\"uid\",\"wd\"]).mean().to_dict()\n",
    "\n",
    "    # uid*wdごとのログの長さ分散\n",
    "    uid_wd2wd_log_len_std = train.groupby([\"uid\", \"wd\", \"d\"]).size().groupby([\"uid\",\"wd\"]).std().to_dict()\n",
    "\n",
    "    # uid*wdごとのログの長さ歪度\n",
    "    uid_wd2wd_log_len_skew = train.groupby([\"uid\", \"wd\", \"d\"]).size().groupby([\"uid\",\"wd\"]).skew().to_dict()\n",
    "\n",
    "    # uidごとのxyユニーク数\n",
    "    uid2xy_nunique = train.groupby([\"uid\"])[\"xy\"].nunique().to_dict()\n",
    "\n",
    "    # uidごとの最頻滞在地の滞在割合\n",
    "    temp_df = train.groupby([\"uid\"])[\"xy\"].value_counts(normalize=True).groupby([\"uid\"]).head(1).to_frame().rename(columns={\"xy\":\"v\"}).reset_index()\n",
    "    uid2top1_ratio = {uid:v for uid, v in zip(temp_df[\"uid\"], temp_df[\"v\"])}\n",
    "\n",
    "    ### test期間の特徴量(コンペ特有でtの系列情報は使えるので使ってよい not leak)\n",
    "    uid_d2log_len = test.groupby([\"uid\", \"d\"]).size().to_dict()\n",
    "    temp_df = test.loc[(test[\"t\"]>=0)&(test['t'] <= 11)].groupby(['uid', 'd']).size().reset_index(name='count')\n",
    "    uid_d2t0_11_cnt = {(uid, t):v for uid, t, v in zip(temp_df[\"uid\"], temp_df[\"d\"] , temp_df[\"count\"])}\n",
    "    temp_df = test.loc[(test[\"t\"]>=12)&(test['t'] <= 23)].groupby(['uid', 'd']).size().reset_index(name='count')\n",
    "    uid_d2t12_23_cnt = {(uid, t):v for uid, t, v in zip(temp_df[\"uid\"], temp_df[\"d\"] , temp_df[\"count\"])}\n",
    "    temp_df = test.loc[(test[\"t\"]>=24)&(test['t'] <= 35)].groupby(['uid', 'd']).size().reset_index(name='count')\n",
    "    uid_d2t24_35_cnt = {(uid, t):v for uid, t, v in zip(temp_df[\"uid\"], temp_df[\"d\"] , temp_df[\"count\"])}\n",
    "    temp_df = test.loc[(test[\"t\"]>=36)&(test['t'] <= 47)].groupby(['uid', 'd']).size().reset_index(name='count')\n",
    "    uid_d2t36_47_cnt = {(uid, t):v for uid, t, v in zip(temp_df[\"uid\"], temp_df[\"d\"] , temp_df[\"count\"])}\n",
    "\n",
    "\n",
    "    ## map\n",
    "    train_df[\"wd_flag\"] = train_df[\"d\"].isin(weekday_list)*1\n",
    "    train_df[\"over_d\"] = train_df[\"d\"] - Config.test_start_day\n",
    "\n",
    "    train_df[\"log_len_median\"] = train_df[\"uid\"].map(uid2log_len_median)\n",
    "    train_df[\"log_len_mean\"] = train_df[\"uid\"].map(uid2log_len_mean)\n",
    "    train_df[\"log_len_std\"] = train_df[\"uid\"].map(uid2log_len_std)\n",
    "    train_df[\"log_len_skew\"] = train_df[\"uid\"].map(uid2log_len_skew)\n",
    "    train_df[\"xy_nunique\"] = train_df[\"uid\"].map(uid2xy_nunique)\n",
    "\n",
    "    def map_vec(key1, key2, dic):\n",
    "        try:\n",
    "            return dic[(key1, key2)]\n",
    "        except KeyError:\n",
    "            return -99999\n",
    "\n",
    "    train_df[\"wd_log_len_median\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid_wd2wd_log_len_median)\n",
    "    train_df[\"wd_log_len_mean\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid_wd2wd_log_len_mean)\n",
    "    train_df[\"wd_log_len_std\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid_wd2wd_log_len_std)\n",
    "    train_df[\"wd_log_len_skew\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid_wd2wd_log_len_skew)\n",
    "    train_df[\"top1_ratio\"] = train_df[\"uid\"].map(uid2top1_ratio)\n",
    "    train_df[\"log_len\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"d\"], uid_d2log_len)\n",
    "    train_df[\"t0_11_cnt\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"d\"], uid_d2t0_11_cnt)\n",
    "    train_df[\"t12_23_cnt\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"d\"], uid_d2t12_23_cnt)\n",
    "    train_df[\"t24_35_cnt\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"d\"], uid_d2t24_35_cnt)\n",
    "    train_df[\"t36_47_cnt\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"d\"], uid_d2t36_47_cnt)\n",
    "\n",
    "    train_df = train_df.fillna(-99999)\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = feature_engineering(train, test, train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答えの算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, days in enumerate([weekday_list, weekend_list]):\n",
    "    part_df = train.loc[train.d.isin(days)]\n",
    "    uids = list(part_df.uid.unique())\n",
    "    # t粒度\n",
    "    t_func = [td002]\n",
    "    uid_t_xy2wgt = calc_wgt_func(part_df, t_func, Config.co_start_day, Config.co_end_day, Config.test_end_day)\n",
    "    test.loc[test.d.isin(days), \"pred_t\"] = map_dict_to_df_t(uid_t_xy2wgt, test.loc[test.d.isin(days)])\n",
    "    # group_t粒度\n",
    "    group_t_func = [td105]\n",
    "    uid_group_t_xy2wgt = calc_wgt_func(part_df, group_t_func, Config.co_start_day, Config.co_end_day, Config.test_end_day)\n",
    "    test.loc[test.d.isin(days), \"pred_group_t\"] = map_dict_to_df_group_t(uid_group_t_xy2wgt, test.loc[test.d.isin(days)])    \n",
    "\n",
    "## pred_tがnullの列はgroup_tでうめる\n",
    "test.loc[test.pred_t.isnull(), \"pred_t\"] = test.loc[test.pred_t.isnull(), \"pred_group_t\"]\n",
    "\n",
    "test[\"pred_t\"] = test.groupby([\"uid\",\"d\"])[\"pred_t\"].ffill()\n",
    "test[\"pred_t\"] = test.groupby([\"uid\",\"d\"])[\"pred_t\"].bfill()\n",
    "\n",
    "test[\"pred_group_t\"] = test.groupby([\"uid\",\"d\"])[\"pred_group_t\"].ffill()\n",
    "test[\"pred_group_t\"] = test.groupby([\"uid\",\"d\"])[\"pred_group_t\"].bfill()\n",
    "\n",
    "\n",
    "# # 予測できていない部分をuidごとの最頻値で埋める\n",
    "uid2most_xy = get_uid2most_xy(train)\n",
    "test.loc[test.pred_t.isnull(), \"pred_t\"] = test.loc[test.pred_t.isnull(), \"uid\"].map(uid2most_xy)\n",
    "test.loc[test.pred_group_t.isnull(), \"pred_group_t\"] = test.loc[test.pred_group_t.isnull(), \"uid\"].map(uid2most_xy)\n",
    "\n",
    "# 後処理\n",
    "test[\"pred_t\"] = test.pred_t.astype(int)\n",
    "test[\"pred_group_t\"] = test.pred_group_t.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred_t_x'] = test['pred_t'].astype(str).str[:-3].astype(int)\n",
    "test['pred_t_y'] = test['pred_t'].astype(str).str[-3:].astype(int)\n",
    "test['pred_group_t_x'] = test['pred_group_t'].astype(str).str[:-3].astype(int)\n",
    "test['pred_group_t_y'] = test['pred_group_t'].astype(str).str[-3:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_geobleu_t(df):\n",
    "    reference = df[[\"d\",\"t\",\"x\",\"y\"]].to_numpy()\n",
    "    generated = df[[\"d\",\"t\",\"pred_t_x\",\"pred_t_y\"]].to_numpy()\n",
    "    return geobleu.calc_geobleu_single_list(generated.tolist(), reference.tolist()) #日付間の並列化不要。ユーザーごとに並列化する。\n",
    "\n",
    "def calc_geobleu_group_t(df):\n",
    "    reference = df[[\"d\",\"t\",\"x\",\"y\"]].to_numpy()\n",
    "    generated = df[[\"d\",\"t\",\"pred_group_t_x\",\"pred_group_t_y\"]].to_numpy()\n",
    "    return geobleu.calc_geobleu_single_list(generated.tolist(), reference.tolist()) #日付間の並列化不要。ユーザーごとに並列化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:12<00:00, 199.06it/s]\n",
      "100%|██████████| 2500/2500 [00:10<00:00, 235.89it/s]\n"
     ]
    }
   ],
   "source": [
    "grouped = test[[\"uid\", \"d\", \"t\", \"x\", \"y\", \"pred_t_x\", \"pred_t_y\"]].groupby(\"uid\")\n",
    "results = Parallel(n_jobs=-1)(delayed(calc_geobleu_t)(df) for _, df in tqdm(grouped))\n",
    "train_df[\"pred_t_geobleu\"] = sum(results, [])\n",
    "\n",
    "grouped = test[[\"uid\", \"d\", \"t\", \"x\", \"y\", \"pred_group_t_x\", \"pred_group_t_y\"]].groupby(\"uid\")\n",
    "results = Parallel(n_jobs=-1)(delayed(calc_geobleu_group_t)(df) for _, df in tqdm(grouped))\n",
    "train_df[\"pred_group_t_geobleu\"] = sum(results, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task2, data CB\n",
      "uid does not exist, saving...\n",
      "d does not exist, saving...\n",
      "wd does not exist, saving...\n",
      "wd_flag does not exist, saving...\n",
      "over_d does not exist, saving...\n",
      "log_len_median does not exist, saving...\n",
      "log_len_mean does not exist, saving...\n",
      "log_len_std does not exist, saving...\n",
      "log_len_skew does not exist, saving...\n",
      "xy_nunique does not exist, saving...\n",
      "wd_log_len_median does not exist, saving...\n",
      "wd_log_len_mean does not exist, saving...\n",
      "wd_log_len_std does not exist, saving...\n",
      "wd_log_len_skew does not exist, saving...\n",
      "top1_ratio does not exist, saving...\n",
      "log_len does not exist, saving...\n",
      "t0_11_cnt does not exist, saving...\n",
      "t12_23_cnt does not exist, saving...\n",
      "t24_35_cnt does not exist, saving...\n",
      "t36_47_cnt does not exist, saving...\n",
      "pred_t_geobleu does not exist, saving...\n",
      "pred_group_t_geobleu does not exist, saving...\n"
     ]
    }
   ],
   "source": [
    "print(f\"{Config.task}, data {Config.data}\")\n",
    "for col in train_df.columns:\n",
    "    if not check_exist_file(f\"{OUTPUT_DIR}/{Config.stage}/{col}.pqt\"):\n",
    "        print(f\"{col} does not exist, saving...\")\n",
    "        train_df[[col]].to_parquet(f\"{OUTPUT_DIR}/{Config.stage}/{col}.pqt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
