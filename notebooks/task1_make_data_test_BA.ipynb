{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import lightgbm as lgb\n",
    "\n",
    "import geobleu\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import importlib\n",
    "from util import *\n",
    "import util\n",
    "\n",
    "from calc_metrices import *\n",
    "import calc_metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "  exp = \"ens\"\n",
    "  task = 'task1'\n",
    "  data = \"BA\"\n",
    "  stage = \"test\"\n",
    "\n",
    "  # candidate hypara. group window_size\n",
    "  window_size = 3\n",
    "\n",
    "  # for time decay\n",
    "  co_start_day = 0\n",
    "  co_end_day = 59\n",
    "\n",
    "  # co-visitation matrix\n",
    "  n_ago = 3\n",
    "  \n",
    "  # train test split. taskごとに要変更\n",
    "  start_uid = 80000\n",
    "  end_uid = 99999\n",
    "  train_start_day = 0\n",
    "  train_end_day = 59\n",
    "  test_start_day = 60\n",
    "  test_end_day = 74\n",
    "\n",
    "  # train, valid split. taskごとに要変更\n",
    "  model_train_start_day = 0\n",
    "  model_train_end_day = 40\n",
    "  model_valid_start_day = 41\n",
    "  model_valid_end_day = 59\n",
    "\n",
    "  seed = 123\n",
    "  debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = f'/root/humob/input/pkl/' #各自データがあるパスに変更の必要あり\n",
    "OUTPUT_DIR = f'/root/humob/pub/data/{Config.task}/{Config.data}/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "  os.makedirs(OUTPUT_DIR)\n",
    "if not os.path.exists(OUTPUT_DIR+Config.stage):\n",
    "  os.makedirs(OUTPUT_DIR+Config.stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : Dataset_load...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Processing : Dataset_load...')\n",
    "print('')\n",
    "\n",
    "# Load dataset\n",
    "df = dataset_load(Config.task, Config.debug, INPUT_DIR)\n",
    "\n",
    "all_days = np.arange(df['d'].nunique())\n",
    "weekend_list = [0, 1, 6, 7, 8, 13, 14, 20, 21, 27, 28, 29, 34, 35, 37, 41, 42, 48, 49, 50, 55, 56, 62, 63, 69, 70,]\n",
    "weekday_list = [day for day in all_days if day not in weekend_list]\n",
    "\n",
    "# preprocess\n",
    "df = extract_uid(df, Config.start_uid, Config.end_uid)\n",
    "df[\"wd\"] = df[\"d\"] % 7\n",
    "df['xy'] = df['x'].astype(str).str.zfill(3) + df['y'].astype(str).str.zfill(3)\n",
    "df['xy'] = df['xy'].astype(int)\n",
    "\n",
    "df = add_cumcount(df)\n",
    "df = add_group_xx(df, window_size=Config.window_size)\n",
    "df, drop_cols = add_lag_mesh(df, Config.n_ago)\n",
    "\n",
    "train, test = train_test_split_func(df, Config.train_start_day, Config.train_end_day, Config.test_start_day, Config.test_end_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(train, test, train_df):\n",
    "    ### train期間の特徴量\n",
    "    # uid*dごとのログの長さ中央値\n",
    "    uid2log_len_median = train.groupby([\"uid\", \"d\"]).size().groupby(\"uid\").median().to_dict()\n",
    "\n",
    "    # uid*dごとのログの長さ平均値\n",
    "    uid2log_len_mean = train.groupby([\"uid\", \"d\"]).size().groupby(\"uid\").mean().to_dict()\n",
    "\n",
    "    # uid*dごとのログの長さ分散\n",
    "    uid2log_len_std = train.groupby([\"uid\", \"d\"]).size().groupby(\"uid\").std().to_dict()\n",
    "\n",
    "    # uid*dごとのログの長さ歪度\n",
    "    uid2log_len_skew = train.groupby([\"uid\", \"d\"]).size().groupby(\"uid\").skew().to_dict()\n",
    "\n",
    "    # uid*wdごとのログの長さ中央値\n",
    "    uid_wd2wd_log_len_median = train.groupby([\"uid\", \"wd\", \"d\"]).size().groupby([\"uid\",\"wd\"]).median().to_dict()\n",
    "\n",
    "    # uid*wdごとのログの長さ平均値\n",
    "    uid_wd2wd_log_len_mean = train.groupby([\"uid\", \"wd\", \"d\"]).size().groupby([\"uid\",\"wd\"]).mean().to_dict()\n",
    "\n",
    "    # uid*wdごとのログの長さ分散\n",
    "    uid_wd2wd_log_len_std = train.groupby([\"uid\", \"wd\", \"d\"]).size().groupby([\"uid\",\"wd\"]).std().to_dict()\n",
    "\n",
    "    # uid*wdごとのログの長さ歪度\n",
    "    uid_wd2wd_log_len_skew = train.groupby([\"uid\", \"wd\", \"d\"]).size().groupby([\"uid\",\"wd\"]).skew().to_dict()\n",
    "\n",
    "    # uidごとのxyユニーク数\n",
    "    uid2xy_nunique = train.groupby([\"uid\"])[\"xy\"].nunique().to_dict()\n",
    "\n",
    "    # uidごとの最頻滞在地の滞在割合\n",
    "    temp_df = train.groupby([\"uid\"])[\"xy\"].value_counts(normalize=True).groupby([\"uid\"]).head(1).to_frame().rename(columns={\"xy\":\"v\"}).reset_index()\n",
    "    uid2top1_ratio = {uid:v for uid, v in zip(temp_df[\"uid\"], temp_df[\"v\"])}\n",
    "\n",
    "    ### test期間の特徴量(コンペ特有でtの系列情報は使えるので使ってよい not leak)\n",
    "    uid_d2log_len = test.groupby([\"uid\", \"d\"]).size().to_dict()\n",
    "    temp_df = test.loc[(test[\"t\"]>=0)&(test['t'] <= 11)].groupby(['uid', 'd']).size().reset_index(name='count')\n",
    "    uid_d2t0_11_cnt = {(uid, t):v for uid, t, v in zip(temp_df[\"uid\"], temp_df[\"d\"] , temp_df[\"count\"])}\n",
    "    temp_df = test.loc[(test[\"t\"]>=12)&(test['t'] <= 23)].groupby(['uid', 'd']).size().reset_index(name='count')\n",
    "    uid_d2t12_23_cnt = {(uid, t):v for uid, t, v in zip(temp_df[\"uid\"], temp_df[\"d\"] , temp_df[\"count\"])}\n",
    "    temp_df = test.loc[(test[\"t\"]>=24)&(test['t'] <= 35)].groupby(['uid', 'd']).size().reset_index(name='count')\n",
    "    uid_d2t24_35_cnt = {(uid, t):v for uid, t, v in zip(temp_df[\"uid\"], temp_df[\"d\"] , temp_df[\"count\"])}\n",
    "    temp_df = test.loc[(test[\"t\"]>=36)&(test['t'] <= 47)].groupby(['uid', 'd']).size().reset_index(name='count')\n",
    "    uid_d2t36_47_cnt = {(uid, t):v for uid, t, v in zip(temp_df[\"uid\"], temp_df[\"d\"] , temp_df[\"count\"])}\n",
    "\n",
    "\n",
    "    ## map\n",
    "    train_df[\"wd_flag\"] = train_df[\"d\"].isin(weekday_list)*1\n",
    "    train_df[\"over_d\"] = train_df[\"d\"] - Config.test_start_day\n",
    "\n",
    "    train_df[\"log_len_median\"] = train_df[\"uid\"].map(uid2log_len_median)\n",
    "    train_df[\"log_len_mean\"] = train_df[\"uid\"].map(uid2log_len_mean)\n",
    "    train_df[\"log_len_std\"] = train_df[\"uid\"].map(uid2log_len_std)\n",
    "    train_df[\"log_len_skew\"] = train_df[\"uid\"].map(uid2log_len_skew)\n",
    "    train_df[\"xy_nunique\"] = train_df[\"uid\"].map(uid2xy_nunique)\n",
    "\n",
    "    def map_vec(key1, key2, dic):\n",
    "        try:\n",
    "            return dic[(key1, key2)]\n",
    "        except KeyError:\n",
    "            return -99999\n",
    "\n",
    "    train_df[\"wd_log_len_median\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid_wd2wd_log_len_median)\n",
    "    train_df[\"wd_log_len_mean\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid_wd2wd_log_len_mean)\n",
    "    train_df[\"wd_log_len_std\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid_wd2wd_log_len_std)\n",
    "    train_df[\"wd_log_len_skew\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid_wd2wd_log_len_skew)\n",
    "    train_df[\"top1_ratio\"] = train_df[\"uid\"].map(uid2top1_ratio)\n",
    "    train_df[\"log_len\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"d\"], uid_d2log_len)\n",
    "    train_df[\"t0_11_cnt\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"d\"], uid_d2t0_11_cnt)\n",
    "    train_df[\"t12_23_cnt\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"d\"], uid_d2t12_23_cnt)\n",
    "    train_df[\"t24_35_cnt\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"d\"], uid_d2t24_35_cnt)\n",
    "    train_df[\"t36_47_cnt\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"d\"], uid_d2t36_47_cnt)\n",
    "\n",
    "    # ------- suzuki add ------- #\n",
    "\n",
    "    ### make dict ###\n",
    "\n",
    "    # uid\n",
    "    uid2xy_var = train.groupby([\"uid\"])[\"xy\"].var().to_dict()\n",
    "    uid2xy_std = train.groupby([\"uid\"])[\"xy\"].std().to_dict()\n",
    "    uid2x_var = train.groupby([\"uid\"])[\"x\"].var().to_dict()\n",
    "    uid2x_std = train.groupby([\"uid\"])[\"x\"].std().to_dict()\n",
    "    uid2y_var = train.groupby([\"uid\"])[\"y\"].var().to_dict()\n",
    "    uid2y_std = train.groupby([\"uid\"])[\"y\"].std().to_dict()\n",
    "\n",
    "    uid2xy_max = train.groupby([\"uid\"])[\"xy\"].max().to_dict()\n",
    "    uid2xy_min = train.groupby([\"uid\"])[\"xy\"].min().to_dict()\n",
    "    uid2x_max = train.groupby([\"uid\"])[\"x\"].max().to_dict()\n",
    "    uid2x_min = train.groupby([\"uid\"])[\"x\"].min().to_dict()\n",
    "    uid2y_max = train.groupby([\"uid\"])[\"y\"].max().to_dict()\n",
    "    uid2y_min = train.groupby([\"uid\"])[\"y\"].min().to_dict()\n",
    "\n",
    "\n",
    "    uid2xy_mean = train.groupby([\"uid\"])[\"xy\"].mean().to_dict()\n",
    "    uid2xy_median = train.groupby([\"uid\"])[\"xy\"].median().to_dict()\n",
    "    uid2x_mean = train.groupby([\"uid\"])[\"x\"].mean().to_dict()\n",
    "    uid2x_median = train.groupby([\"uid\"])[\"x\"].median().to_dict()\n",
    "    uid2y_mean = train.groupby([\"uid\"])[\"y\"].mean().to_dict()\n",
    "    uid2y_median = train.groupby([\"uid\"])[\"y\"].median().to_dict()\n",
    "\n",
    "\n",
    "    # uid * wd\n",
    "    uid2xy_wd_var = train.groupby([\"uid\", \"wd\"])['xy'].var().to_dict()\n",
    "    uid2xy_wd_std = train.groupby([\"uid\", \"wd\"])['xy'].std().to_dict()\n",
    "    uid2x_wd_var = train.groupby([\"uid\", \"wd\"])['x'].var().to_dict()\n",
    "    uid2x_wd_std = train.groupby([\"uid\", \"wd\"])['x'].std().to_dict()\n",
    "    uid2y_wd_var = train.groupby([\"uid\", \"wd\"])['y'].var().to_dict()\n",
    "    uid2y_wd_std = train.groupby([\"uid\", \"wd\"])['y'].std().to_dict()\n",
    "\n",
    "    uid2xy_wd_max = train.groupby([\"uid\", \"wd\"])[\"xy\"].max().to_dict()\n",
    "    uid2xy_wd_min = train.groupby([\"uid\", \"wd\"])[\"xy\"].min().to_dict()\n",
    "    uid2x_wd_max = train.groupby([\"uid\", \"wd\"])[\"x\"].max().to_dict()\n",
    "    uid2x_wd_min = train.groupby([\"uid\", \"wd\"])[\"x\"].min().to_dict()\n",
    "    uid2y_wd_max = train.groupby([\"uid\", \"wd\"])[\"y\"].max().to_dict()\n",
    "    uid2y_wd_min = train.groupby([\"uid\", \"wd\"])[\"y\"].min().to_dict()\n",
    "\n",
    "    uid2xy_wd_mean = train.groupby([\"uid\", \"wd\"])[\"xy\"].mean().to_dict()\n",
    "    uid2xy_wd_median = train.groupby([\"uid\", \"wd\"])[\"xy\"].median().to_dict()\n",
    "    uid2x_wd_mean = train.groupby([\"uid\", \"wd\"])[\"x\"].mean().to_dict()\n",
    "    uid2x_wd_median = train.groupby([\"uid\", \"wd\"])[\"x\"].median().to_dict()\n",
    "    uid2y_wd_mean = train.groupby([\"uid\", \"wd\"])[\"y\"].mean().to_dict()\n",
    "    uid2y_wd_median = train.groupby([\"uid\", \"wd\"])[\"y\"].median().to_dict()\n",
    "\n",
    "    ### make feature ###\n",
    "\n",
    "    # uid\n",
    "    train_df[\"xy_var\"] = train_df[\"uid\"].map(uid2xy_var)\n",
    "    train_df[\"xy_std\"] = train_df[\"uid\"].map(uid2xy_std)\n",
    "    train_df[\"x_var\"] = train_df[\"uid\"].map(uid2x_var)\n",
    "    train_df[\"x_std\"] = train_df[\"uid\"].map(uid2x_std)\n",
    "    train_df[\"y_var\"] = train_df[\"uid\"].map(uid2y_var)\n",
    "    train_df[\"y_std\"] = train_df[\"uid\"].map(uid2y_std)\n",
    "\n",
    "\n",
    "    train_df[\"uid2xy_max\"] = train_df[\"uid\"].map(uid2xy_max)\n",
    "    train_df[\"uid2xy_min\"] = train_df[\"uid\"].map(uid2xy_min)\n",
    "    train_df[\"uid2x_max\"] = train_df[\"uid\"].map(uid2x_max)\n",
    "    train_df[\"uid2x_min\"] = train_df[\"uid\"].map(uid2x_min)\n",
    "    train_df[\"uid2y_max\"] = train_df[\"uid\"].map(uid2y_max)\n",
    "    train_df[\"uid2y_min\"] = train_df[\"uid\"].map(uid2y_min)\n",
    "\n",
    "    train_df[\"uid2xy_mean\"] = train_df[\"uid\"].map(uid2xy_mean)\n",
    "    train_df[\"uid2xy_median\"] = train_df[\"uid\"].map(uid2xy_median)\n",
    "    train_df[\"uid2x_mean\"] = train_df[\"uid\"].map(uid2x_mean)\n",
    "    train_df[\"uid2x_median\"] = train_df[\"uid\"].map(uid2x_median)\n",
    "    train_df[\"uid2y_mean\"] = train_df[\"uid\"].map(uid2y_mean)\n",
    "    train_df[\"uid2y_median\"] = train_df[\"uid\"].map(uid2y_median)\n",
    "\n",
    "\n",
    "    # uid * wd\n",
    "    train_df[\"uid2xy_wd_var\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2xy_wd_var)\n",
    "    train_df[\"uid2xy_wd_std\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2xy_wd_std)\n",
    "    train_df[\"uid2x_wd_var\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2x_wd_var)\n",
    "    train_df[\"uid2x_wd_std\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2x_wd_std)\n",
    "    train_df[\"uid2y_wd_var\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2y_wd_var)\n",
    "    train_df[\"uid2y_wd_std\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2y_wd_std)\n",
    "\n",
    "    train_df[\"uid2xy_wd_max\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2xy_wd_max)\n",
    "    train_df[\"uid2xy_wd_min\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2xy_wd_min)\n",
    "    train_df[\"uid2x_wd_max\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2x_wd_max)\n",
    "    train_df[\"uid2x_wd_min\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2x_wd_min)\n",
    "    train_df[\"uid2y_wd_max\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2y_wd_max)\n",
    "    train_df[\"uid2y_wd_min\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2y_wd_min)\n",
    "\n",
    "    train_df[\"uid2xy_wd_mean\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2xy_wd_mean)\n",
    "    train_df[\"uid2xy_wd_median\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2xy_wd_median)\n",
    "    train_df[\"uid2x_wd_mean\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2x_wd_mean)\n",
    "    train_df[\"uid2x_wd_median\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2x_wd_median)\n",
    "    train_df[\"uid2y_wd_mean\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2y_wd_mean)\n",
    "    train_df[\"uid2y_wd_median\"] = np.vectorize(map_vec)(train_df[\"uid\"], train_df[\"wd\"], uid2y_wd_median)\n",
    "\n",
    "\n",
    "    ### diff ###\n",
    "    # uid\n",
    "    train_df[\"uid2xy_diff_max_min\"] = train_df[\"uid2xy_max\"] - train_df[\"uid2xy_min\"]\n",
    "    train_df[\"uid2x_diff_max_min\"] = train_df[\"uid2x_max\"] - train_df[\"uid2x_min\"]\n",
    "    train_df[\"uid2y_diff_max_min\"] = train_df[\"uid2y_max\"] - train_df[\"uid2y_min\"]\n",
    "\n",
    "    train_df[\"uid2xy_diff_max_mean\"] = train_df[\"uid2xy_max\"] - train_df[\"uid2xy_mean\"]\n",
    "    train_df[\"uid2x_diff_max_mean\"] = train_df[\"uid2x_max\"] - train_df[\"uid2x_mean\"]\n",
    "    train_df[\"uid2y_diff_max_mean\"] = train_df[\"uid2y_max\"] - train_df[\"uid2y_mean\"]\n",
    "\n",
    "    train_df[\"uid2xy_diff_mean_min\"] = train_df[\"uid2xy_mean\"] - train_df[\"uid2xy_min\"]\n",
    "    train_df[\"uid2x_diff_mean_min\"] = train_df[\"uid2x_mean\"] - train_df[\"uid2x_min\"]\n",
    "    train_df[\"uid2y_diff_mean_min\"] = train_df[\"uid2y_mean\"] - train_df[\"uid2y_min\"]\n",
    "\n",
    "    train_df[\"uid2xy_diff_max_median\"] = train_df[\"uid2xy_max\"] - train_df[\"uid2xy_median\"]\n",
    "    train_df[\"uid2x_diff_max_median\"] = train_df[\"uid2x_max\"] - train_df[\"uid2x_median\"]\n",
    "    train_df[\"uid2y_diff_max_median\"] = train_df[\"uid2y_max\"] - train_df[\"uid2y_median\"]\n",
    "\n",
    "    train_df[\"uid2xy_diff_median_min\"] = train_df[\"uid2xy_median\"] - train_df[\"uid2xy_min\"]\n",
    "    train_df[\"uid2x_diff_median_min\"] = train_df[\"uid2x_median\"] - train_df[\"uid2x_min\"]\n",
    "    train_df[\"uid2y_diff_median_min\"] = train_df[\"uid2y_median\"] - train_df[\"uid2y_min\"]\n",
    "\n",
    "    # uid * wd\n",
    "    train_df[\"uid2xy_wd_diff_max_min\"] = train_df[\"uid2xy_wd_max\"] - train_df[\"uid2xy_wd_min\"]\n",
    "    train_df[\"uid2x_wd_diff_max_min\"] = train_df[\"uid2x_wd_max\"] - train_df[\"uid2x_wd_min\"]\n",
    "    train_df[\"uid2y_wd_diff_max_min\"] = train_df[\"uid2y_wd_max\"] - train_df[\"uid2y_wd_min\"]\n",
    "\n",
    "    train_df[\"uid2xy_wd_diff_max_mean\"] = train_df[\"uid2xy_wd_max\"] - train_df[\"uid2xy_wd_mean\"]\n",
    "    train_df[\"uid2x_wd_diff_max_mean\"] = train_df[\"uid2x_wd_max\"] - train_df[\"uid2x_wd_mean\"]\n",
    "    train_df[\"uid2y_wd_diff_max_mean\"] = train_df[\"uid2y_wd_max\"] - train_df[\"uid2y_wd_mean\"]\n",
    "\n",
    "    train_df[\"uid2xy_wd_diff_mean_min\"] = train_df[\"uid2xy_wd_mean\"] - train_df[\"uid2xy_wd_min\"]\n",
    "    train_df[\"uid2x_wd_diff_mean_min\"] = train_df[\"uid2x_wd_mean\"] - train_df[\"uid2x_wd_min\"]\n",
    "    train_df[\"uid2y_wd_diff_mean_min\"] = train_df[\"uid2y_wd_mean\"] - train_df[\"uid2y_wd_min\"]\n",
    "\n",
    "    train_df[\"uid2xy_wd_diff_max_median\"] = train_df[\"uid2xy_wd_max\"] - train_df[\"uid2xy_median\"]\n",
    "    train_df[\"uid2x_wd_diff_max_median\"] = train_df[\"uid2x_wd_max\"] - train_df[\"uid2x_median\"]\n",
    "    train_df[\"uid2y_wd_diff_max_median\"] = train_df[\"uid2y_wd_max\"] - train_df[\"uid2y_median\"]\n",
    "\n",
    "    train_df[\"uid2xy_wd_diff_median_min\"] = train_df[\"uid2xy_wd_median\"] - train_df[\"uid2xy_wd_min\"]\n",
    "    train_df[\"uid2x_wd_diff_median_min\"] = train_df[\"uid2x_wd_median\"] - train_df[\"uid2x_wd_min\"]\n",
    "    train_df[\"uid2y_wd_diff_median_min\"] = train_df[\"uid2y_wd_median\"] - train_df[\"uid2y_wd_min\"]\n",
    "\n",
    "\n",
    "    ### 距離 ###\n",
    "\n",
    "    # uid距離を計算\n",
    "    def uid_calculate_distance(row):\n",
    "        x1, y1 = row['uid2x_min'], row['uid2y_min']\n",
    "        x2, y2 = row['uid2x_max'], row['uid2y_max']\n",
    "        distance = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "        return distance\n",
    "    train_df['uid_xy_distance'] = train_df[['uid2x_min','uid2x_max','uid2y_min','uid2y_max']].apply(uid_calculate_distance, axis=1)\n",
    "\n",
    "    # uid_wd距離を計算\n",
    "    def uid_wd_calculate_distance(row):\n",
    "        x1, y1 = row['uid2x_wd_min'], row['uid2y_wd_min']\n",
    "        x2, y2 = row['uid2x_wd_max'], row['uid2y_wd_max']\n",
    "        distance = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "        return distance\n",
    "    train_df['uid_wd_xy_distance'] = train_df[['uid2x_wd_min','uid2x_wd_max','uid2y_wd_min','uid2y_wd_max']].apply(uid_wd_calculate_distance, axis=1)\n",
    "\n",
    "    # ------------------------------- #\n",
    "\n",
    "    train_df = train_df.fillna(-99999)\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = test.groupby([\"uid\", \"d\"]).head(1)[[\"uid\", \"d\", \"wd\"]].reset_index(drop=True)\n",
    "train_df = feature_engineering(train, test, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, days in enumerate([weekday_list, weekend_list]):\n",
    "    part_df = train.loc[train.d.isin(days)]\n",
    "    uids = list(part_df.uid.unique())\n",
    "    # t粒度\n",
    "    t_func = [td002]\n",
    "    uid_t_xy2wgt = calc_wgt_func(part_df, t_func, Config.co_start_day, Config.co_end_day, Config.test_end_day)\n",
    "    test.loc[test.d.isin(days), \"pred_t\"] = map_dict_to_df_t(uid_t_xy2wgt, test.loc[test.d.isin(days)])\n",
    "    # group_t粒度\n",
    "    group_t_func = [td105]\n",
    "    uid_group_t_xy2wgt = calc_wgt_func(part_df, group_t_func, Config.co_start_day, Config.co_end_day, Config.test_end_day)\n",
    "    test.loc[test.d.isin(days), \"pred_group_t\"] = map_dict_to_df_group_t(uid_group_t_xy2wgt, test.loc[test.d.isin(days)])    \n",
    "\n",
    "## pred_tがnullの列はgroup_tでうめる\n",
    "test.loc[test.pred_t.isnull(), \"pred_t\"] = test.loc[test.pred_t.isnull(), \"pred_group_t\"]\n",
    "\n",
    "test[\"pred_t\"] = test.groupby([\"uid\",\"d\"])[\"pred_t\"].ffill()\n",
    "test[\"pred_t\"] = test.groupby([\"uid\",\"d\"])[\"pred_t\"].bfill()\n",
    "\n",
    "test[\"pred_group_t\"] = test.groupby([\"uid\",\"d\"])[\"pred_group_t\"].ffill()\n",
    "test[\"pred_group_t\"] = test.groupby([\"uid\",\"d\"])[\"pred_group_t\"].bfill()\n",
    "\n",
    "\n",
    "# # 予測できていない部分をuidごとの最頻値で埋める\n",
    "uid2most_xy = get_uid2most_xy(train)\n",
    "test.loc[test.pred_t.isnull(), \"pred_t\"] = test.loc[test.pred_t.isnull(), \"uid\"].map(uid2most_xy)\n",
    "test.loc[test.pred_group_t.isnull(), \"pred_group_t\"] = test.loc[test.pred_group_t.isnull(), \"uid\"].map(uid2most_xy)\n",
    "\n",
    "# 後処理\n",
    "test[\"pred_t\"] = test.pred_t.astype(int)\n",
    "test[\"pred_group_t\"] = test.pred_group_t.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred_t_x'] = test['pred_t'].astype(str).str[:-3].astype(int)\n",
    "test['pred_t_y'] = test['pred_t'].astype(str).str[-3:].astype(int)\n",
    "test['pred_group_t_x'] = test['pred_group_t'].astype(str).str[:-3].astype(int)\n",
    "test['pred_group_t_y'] = test['pred_group_t'].astype(str).str[-3:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_geobleu_t(df):\n",
    "    reference = df[[\"d\",\"t\",\"x\",\"y\"]].to_numpy()\n",
    "    generated = df[[\"d\",\"t\",\"pred_t_x\",\"pred_t_y\"]].to_numpy()\n",
    "    return geobleu.calc_geobleu_single_list(generated.tolist(), reference.tolist()) #日付間の並列化不要。ユーザーごとに並列化する。\n",
    "\n",
    "def calc_geobleu_group_t(df):\n",
    "    reference = df[[\"d\",\"t\",\"x\",\"y\"]].to_numpy()\n",
    "    generated = df[[\"d\",\"t\",\"pred_group_t_x\",\"pred_group_t_y\"]].to_numpy()\n",
    "    return geobleu.calc_geobleu_single_list(generated.tolist(), reference.tolist()) #日付間の並列化不要。ユーザーごとに並列化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:41<00:00, 196.54it/s]\n",
      "100%|██████████| 20000/20000 [01:38<00:00, 202.79it/s]\n"
     ]
    }
   ],
   "source": [
    "grouped = test[[\"uid\", \"d\", \"t\", \"x\", \"y\", \"pred_t_x\", \"pred_t_y\"]].groupby(\"uid\")\n",
    "results = Parallel(n_jobs=-1)(delayed(calc_geobleu_t)(df) for _, df in tqdm(grouped))\n",
    "train_df[\"pred_t_geobleu\"] = sum(results, [])\n",
    "\n",
    "grouped = test[[\"uid\", \"d\", \"t\", \"x\", \"y\", \"pred_group_t_x\", \"pred_group_t_y\"]].groupby(\"uid\")\n",
    "results = Parallel(n_jobs=-1)(delayed(calc_geobleu_group_t)(df) for _, df in tqdm(grouped))\n",
    "train_df[\"pred_group_t_geobleu\"] = sum(results, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1, data BA\n",
      "uid does not exist, saving...\n",
      "d does not exist, saving...\n",
      "wd does not exist, saving...\n",
      "wd_flag does not exist, saving...\n",
      "over_d does not exist, saving...\n",
      "log_len_median does not exist, saving...\n",
      "log_len_mean does not exist, saving...\n",
      "log_len_std does not exist, saving...\n",
      "log_len_skew does not exist, saving...\n",
      "xy_nunique does not exist, saving...\n",
      "wd_log_len_median does not exist, saving...\n",
      "wd_log_len_mean does not exist, saving...\n",
      "wd_log_len_std does not exist, saving...\n",
      "wd_log_len_skew does not exist, saving...\n",
      "top1_ratio does not exist, saving...\n",
      "log_len does not exist, saving...\n",
      "t0_11_cnt does not exist, saving...\n",
      "t12_23_cnt does not exist, saving...\n",
      "t24_35_cnt does not exist, saving...\n",
      "t36_47_cnt does not exist, saving...\n",
      "xy_var does not exist, saving...\n",
      "xy_std does not exist, saving...\n",
      "x_var does not exist, saving...\n",
      "x_std does not exist, saving...\n",
      "y_var does not exist, saving...\n",
      "y_std does not exist, saving...\n",
      "uid2xy_max does not exist, saving...\n",
      "uid2xy_min does not exist, saving...\n",
      "uid2x_max does not exist, saving...\n",
      "uid2x_min does not exist, saving...\n",
      "uid2y_max does not exist, saving...\n",
      "uid2y_min does not exist, saving...\n",
      "uid2xy_mean does not exist, saving...\n",
      "uid2xy_median does not exist, saving...\n",
      "uid2x_mean does not exist, saving...\n",
      "uid2x_median does not exist, saving...\n",
      "uid2y_mean does not exist, saving...\n",
      "uid2y_median does not exist, saving...\n",
      "uid2xy_wd_var does not exist, saving...\n",
      "uid2xy_wd_std does not exist, saving...\n",
      "uid2x_wd_var does not exist, saving...\n",
      "uid2x_wd_std does not exist, saving...\n",
      "uid2y_wd_var does not exist, saving...\n",
      "uid2y_wd_std does not exist, saving...\n",
      "uid2xy_wd_max does not exist, saving...\n",
      "uid2xy_wd_min does not exist, saving...\n",
      "uid2x_wd_max does not exist, saving...\n",
      "uid2x_wd_min does not exist, saving...\n",
      "uid2y_wd_max does not exist, saving...\n",
      "uid2y_wd_min does not exist, saving...\n",
      "uid2xy_wd_mean does not exist, saving...\n",
      "uid2xy_wd_median does not exist, saving...\n",
      "uid2x_wd_mean does not exist, saving...\n",
      "uid2x_wd_median does not exist, saving...\n",
      "uid2y_wd_mean does not exist, saving...\n",
      "uid2y_wd_median does not exist, saving...\n",
      "uid2xy_diff_max_min does not exist, saving...\n",
      "uid2x_diff_max_min does not exist, saving...\n",
      "uid2y_diff_max_min does not exist, saving...\n",
      "uid2xy_diff_max_mean does not exist, saving...\n",
      "uid2x_diff_max_mean does not exist, saving...\n",
      "uid2y_diff_max_mean does not exist, saving...\n",
      "uid2xy_diff_mean_min does not exist, saving...\n",
      "uid2x_diff_mean_min does not exist, saving...\n",
      "uid2y_diff_mean_min does not exist, saving...\n",
      "uid2xy_diff_max_median does not exist, saving...\n",
      "uid2x_diff_max_median does not exist, saving...\n",
      "uid2y_diff_max_median does not exist, saving...\n",
      "uid2xy_diff_median_min does not exist, saving...\n",
      "uid2x_diff_median_min does not exist, saving...\n",
      "uid2y_diff_median_min does not exist, saving...\n",
      "uid2xy_wd_diff_max_min does not exist, saving...\n",
      "uid2x_wd_diff_max_min does not exist, saving...\n",
      "uid2y_wd_diff_max_min does not exist, saving...\n",
      "uid2xy_wd_diff_max_mean does not exist, saving...\n",
      "uid2x_wd_diff_max_mean does not exist, saving...\n",
      "uid2y_wd_diff_max_mean does not exist, saving...\n",
      "uid2xy_wd_diff_mean_min does not exist, saving...\n",
      "uid2x_wd_diff_mean_min does not exist, saving...\n",
      "uid2y_wd_diff_mean_min does not exist, saving...\n",
      "uid2xy_wd_diff_max_median does not exist, saving...\n",
      "uid2x_wd_diff_max_median does not exist, saving...\n",
      "uid2y_wd_diff_max_median does not exist, saving...\n",
      "uid2xy_wd_diff_median_min does not exist, saving...\n",
      "uid2x_wd_diff_median_min does not exist, saving...\n",
      "uid2y_wd_diff_median_min does not exist, saving...\n",
      "uid_xy_distance does not exist, saving...\n",
      "uid_wd_xy_distance does not exist, saving...\n",
      "pred_t_geobleu does not exist, saving...\n",
      "pred_group_t_geobleu does not exist, saving...\n"
     ]
    }
   ],
   "source": [
    "print(f\"{Config.task}, data {Config.data}\")\n",
    "for col in train_df.columns:\n",
    "    if not check_exist_file(f\"{OUTPUT_DIR}/{Config.stage}/{col}.pqt\"):\n",
    "        print(f\"{col} does not exist, saving...\")\n",
    "        train_df[[col]].to_parquet(f\"{OUTPUT_DIR}/{Config.stage}/{col}.pqt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
